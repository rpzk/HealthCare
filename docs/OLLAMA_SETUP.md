# ü§ñ Configura√ß√£o do Ollama para IA Local

Este guia explica como configurar e utilizar o Ollama para fornecer capacidades de IA local ao sistema HealthCare, eliminando a depend√™ncia de servi√ßos de IA em nuvem e protegendo a privacidade dos dados.

## üìã O que √© o Ollama?

[Ollama](https://ollama.com/) √© uma ferramenta de c√≥digo aberto que permite executar modelos de linguagem de grande porte (LLMs) localmente em seu pr√≥prio hardware. Isso oferece v√°rias vantagens:

- **Privacidade**: Os dados nunca saem do seu ambiente
- **Sem custos de API**: Nenhuma taxa por token ou limite de uso
- **Controle total**: Voc√™ decide quais modelos usar e como configur√°-los
- **Opera√ß√£o offline**: Funciona sem acesso √† internet

## üöÄ Modelos Suportados

O HealthCare est√° configurado para usar o modelo `llama3` por padr√£o, mas voc√™ pode escolher qualquer um dos modelos dispon√≠veis na [biblioteca do Ollama](https://ollama.com/library), incluindo:

- `llama3` - Recomendado para uso geral (menor e mais r√°pido)
- `llama3:8b` - Boa combina√ß√£o de tamanho e qualidade
- `llama3:70b` - Melhor qualidade (requer mais recursos)
- `mistral` - Bom equil√≠brio entre efici√™ncia e qualidade
- `phi` - Modelo leve com bom desempenho
- `gemma` - Modelo alternativo da Google
- `llama2` - Modelos anteriores (se preferir)

## üíª Requisitos de Hardware

Para executar o Ollama eficientemente:

- **CPU**: M√≠nimo 4 n√∫cleos, recomendado 8+ n√∫cleos
- **RAM**: M√≠nimo 8GB, recomendado 16GB+
- **GPU**: Opcional, mas recomendado para melhor desempenho (NVIDIA com CUDA)
- **Armazenamento**: 10-50GB dependendo dos modelos instalados

## üîß Configura√ß√£o

### 1. Vari√°veis de Ambiente

O HealthCare j√° est√° configurado para usar o Ollama. No arquivo `.env`:

```env
# Ollama (IA Local)
OLLAMA_URL=http://ollama:11434  # URL do servi√ßo Ollama no Docker
OLLAMA_MODEL=llama3  # Modelo a ser usado
```

### 2. Inicializa√ß√£o

O script `start-with-ollama.sh` j√° configura automaticamente o ambiente:

```bash
# Iniciar tudo com um comando
npm run start:ollama

# Ou para incluir dados de exemplo
npm run start:ollama:seed
```

### 3. Trocando de Modelo

Para usar um modelo diferente:

1. Edite o arquivo `.env` e atualize o valor de `OLLAMA_MODEL`
2. Execute: `docker-compose exec ollama ollama pull nome-do-modelo`
3. Reinicie a aplica√ß√£o: `docker-compose restart app`

## üîç Solu√ß√£o de Problemas

### Modelo n√£o est√° carregando

Verifique se o modelo foi baixado corretamente:

```bash
docker-compose exec ollama ollama list
```

Se o modelo n√£o aparece, baixe-o manualmente:

```bash
docker-compose exec ollama ollama pull llama3
```

### Desempenho lento

- Modelos menores (como `phi:2.7b` ou `llama3:8b`) s√£o mais r√°pidos
- Considere habilitar suporte a GPU na configura√ß√£o do Docker
- Aumente a mem√≥ria dispon√≠vel para o Docker

## üõ°Ô∏è Seguran√ßa e Privacidade

Com o Ollama, todos os dados processados pela IA permanecem em seu ambiente local, eliminando os riscos associados a servi√ßos de IA em nuvem como:

- Vazamento de dados sens√≠veis
- Treinamento do modelo em dados confidenciais
- Depend√™ncia de conectividade com internet
- Custos vari√°veis baseados em uso

Esta abordagem est√° em total conformidade com as melhores pr√°ticas de privacidade e seguran√ßa de dados m√©dicos.
